{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceAligmentRL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "krSN1fAx8eSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, remember to upload the environment."
      ]
    },
    {
      "metadata": {
        "id": "FKT_3UWwzSB7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZING WITH REINFORCEMENT LEARNING\n",
        "\n",
        "Inspired by: https://github.com/llSourcell/deep_q_learning/blob/master/03_PlayingAgent.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "NhQJzHMM8KiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import environment\n",
        "import random\n",
        "import sys\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from collections import deque            # For storing moves\n",
        "\n",
        "env = environment.SequenceAlignmentEnvironment()\n",
        "ALPHA = 0.01 #learning rate\n",
        "EPSILON = 1.0 #chance that a creative action is taken\n",
        "GAMMA = 0.7 #discount factor\n",
        "\n",
        "OBSERVETIME = 100000 # Number of timesteps we will be acting on the game and observing results\n",
        "MBSIZE = 10000 # Learning minibatch size\n",
        "\n",
        "# Create network. Input is two consecutive game states, output is Q-values of the possible moves.\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_shape=(201,), init='uniform', activation='relu'))\n",
        "model.add(Dense(18, init='uniform', activation='relu'))\n",
        "model.add(Dense(10, init='uniform', activation='relu'))\n",
        "model.add(Dense(env.TOTALACTIONS, init='uniform', activation='linear'))    # Same number of outputs as possible actions\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# FIRST STEP: Knowing what each action does (Observing)\n",
        "\n",
        "D = deque()                                # Register where the actions will be stored\n",
        "\n",
        "observation = env.reset().reshape((1,201))\n",
        "done = False\n",
        "for t in range(OBSERVETIME):\n",
        "    sys.stdout.write('\\r' + str(t*100/OBSERVETIME) + '%')\n",
        "    if np.random.rand() <= EPSILON:\n",
        "        action = env.randomAction()\n",
        "    else:\n",
        "        Q = model.predict(observation)\n",
        "        action = np.argmax(Q)\n",
        "    \n",
        "    observation_new, reward, done = env.step(action)\n",
        "    observation_new = observation_new.reshape((1,201))\n",
        "    \n",
        "    D.append((observation, action, reward, observation_new, done))\n",
        "    \n",
        "    observation = observation_new\n",
        "    \n",
        "    if done:\n",
        "        observation = env.reset().reshape((1,201))\n",
        "print('\\nObserving Finished')\n",
        "\n",
        "# SECOND STEP: Learning from the observations (Experience replay)\n",
        "\n",
        "minibatch = random.sample(D, MBSIZE)                              # Sample some moves\n",
        "\n",
        "inputs = np.zeros((MBSIZE,201,))\n",
        "targets = np.zeros((MBSIZE, env.TOTALACTIONS))\n",
        "\n",
        "for i in range(0, MBSIZE):\n",
        "    sys.stdout.write('\\r' + str(i*100/MBSIZE) + '%')\n",
        "    observation = minibatch[i][0]\n",
        "    action = minibatch[i][1]\n",
        "    reward = minibatch[i][2]\n",
        "    observation_new = minibatch[i][3]\n",
        "    done = minibatch[i][4]\n",
        "    \n",
        "    #Build Bellman equation for the Q function\n",
        "    inputs[i] = observation\n",
        "    targets[i] = model.predict(observation)\n",
        "    \n",
        "    #future (discounted) reward for entering the new state\n",
        "    Q_sa = model.predict(observation_new)\n",
        "    \n",
        "    #changing the predictions to their optimal values\n",
        "    #if the model predicted [0.1, 0.2, 0.1] but action 2 reward for the given state should be 0.3, it's changed to [0.1, 0.3, 0.1]\n",
        "    if done:\n",
        "        targets[i][action] = reward\n",
        "    else:\n",
        "        targets[i][action] = reward + (GAMMA * np.max(Q_sa))\n",
        "\n",
        "    #Train network to output the Q function\n",
        "    model.train_on_batch(inputs, targets)\n",
        "print('\\nLearning Finished')\n",
        "\n",
        "# THIRD STEP: Play!\n",
        "observation = env.reset().reshape((1,201))\n",
        "seq1 = np.array([1,3,2,4,2,3,3,3,1,3,2,1,2,3,2,2,1,4,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "seq2 = np.array([3,2,4,2,3,3,3,1,3,2,1,2,3,2,2,1,4,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
        "env.customInput(seq1, seq2)\n",
        "done = False\n",
        "tot_reward = 0.0\n",
        "while not done:\n",
        "    Q = model.predict(observation)\n",
        "    action = np.argmax(Q)         \n",
        "    observation, reward, done = env.step(action)\n",
        "    observation = observation.reshape((1,201))\n",
        "    tot_reward += reward\n",
        "sys.stdout.write(env.display()[1])\n",
        "print('Game ended! Total reward: {}'.format(tot_reward))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}